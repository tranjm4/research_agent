{"execution_time": 1.0940994170377962, "execution_date": "2025-07-17 02:43:03", "output": "'Document 1: In this study, we conducted a comprehensive data collection on the 2022 Qatar FIFA World Cup event and used a multilayer network approach to visualize the main topics, while considering their context and meaning relationships. We structured the data into layers that corresponded with the stages of the tournament and utilized Gephi software to generate the multilayer networks. Our visualizations displayed both the relationships between topics and words, showing the word-context relationship, as well as the dynamics and changes over time by layer of the most frequently discussed topics. (https://arxiv.org/abs/2401.12228)\\nDocument 2: Twitter is a social media platform bridging most countries and allows real-time news discovery. Since the tweets on Twitter are usually short and express public feelings, thus provide a source for opinion mining and sentiment analysis for global events. This paper proposed an effective solution, in providing a sentiment on tweets related to the FIFA World Cup. At least 130k tweets, as the first in the community, are collected and implemented as a dataset to evaluate the performance of the proposed machine learning solution. These tweets are collected with the related hashtags and keywords of the Qatar World Cup 2022. The Vader algorithm is used in this paper for sentiment analysis. Through the machine learning method and collected Twitter tweets, we discovered the sentiments and fun facts of several aspects important to the period before the World Cup. The result shows people are positive to the opening of the World Cup. (https://arxiv.org/abs/2306.16049)\\nDocument 3: Qualifications for several world championships in sports are organised such that distinct sets of teams play in their own tournament for a predetermined number of slots. Inspired by a recent work studying the problem with the tools from the literature on fair allocation, this paper provides an alternative approach based on historical matches between these sets of teams. We focus on the FIFA World Cup due to the existence of an official rating system and its recent expansion to 48 teams, as well as to allow for a comparison with the already suggested allocations. Our proposal extends the methodology of the FIFA World Ranking to compare the strengths of five confederations. Various allocations are presented depending on the length of the sample, the set of teams considered, as well as the frequency of rating updates. The results show that more European and South American teams should play in the FIFA World Cup. The ranking of continents by the number of deserved slots is different from the ranking implied by FIFA policy. We recommend allocating at least some slots transparently, based on historical performances, similar to the access list of the UEFA Champions League. (https://arxiv.org/abs/2310.19100)\\nDocument 4: The Fifth International Conference on Applied Category Theory took place at the University of Strathclyde in Glasgow, Scotland on 18-22 July 2022. This conference follows the previous meetings at Leiden (2018), Oxford (2019), MIT (2020, fully online), and Cambridge (2021). The conference comprised 59 contributed talks, a poster session, an industry showcase session, and a session where junior researchers who had attended the Adjoint School presented the results of their research at the school. Information regarding the conference may be found at (https://msp.cis.strath.ac.uk/act2022).\\n  The contributions to ACT2022 ranged from pure to applied and included contributions in a wide range of disciplines in science and engineering. ACT2022 included talks in linguistics, functional programming, classical mechanics, quantum physics, probability theory, electrical engineering, epidemiology, thermodynamics, engineering, and logic. ACT2022 was sponsored by Huawei, Protocol Labs, Cambridge Quantum, Conexus, Topos, and SICSA (Scottish Informatics and Computer Science Alliance).\\n  Submission to ACT2022 had three tracks: extended abstracts, software demonstrations, and proceedings. The extended abstract and software demonstration submissions had a page limit of 2 pages, and the proceedings track had a page limit of 14 pages. Only papers submitted to the proceedings track were considered for publication in this volume. In total, there were 97 submissions, of which 59 were accepted for presentation and 24 for publication in this volume. Publication of accepted submissions in the proceedings was determined by personal choice of the authors and not based on quality. Each submission received a review from three different members of the programming committee, and papers were selected based on discussion and consensus by these reviewers. (https://arxiv.org/abs/2307.15519)\\nDocument 5: This volume contains the proceedings of DCM 2023, the 13th International Workshop on Developments in Computational Models held on 2 July 2023 in Rome, Italy. DCM 2023 was organised as a one-day satellite event of FSCD 2023, the 8th International Conference on Formal Structures for Computation and Deduction. The aim of this workshop is to bring together researchers who are currently developing new computation models or new features for traditional computation models, in order to foster their interaction, to provide a forum for presenting new ideas and work in progress, and to enable newcomers to learn about current activities in this area. (https://arxiv.org/abs/2409.19298)'", "args": "('Club World Cup 2025 winner',)", "kwargs": "{}", "model_name": "cross-encoder/ms-marco-MiniLM-L-12-v2", "type": "retriever", "version_name": "retriever/cross-encoder/ms-marco-MiniLM-L-12-v2/v1"}
{"execution_time": 1.7164451659773476, "execution_date": "2025-07-17 02:52:48", "output": "'Document 1: With the rapid development of artificial intelligence technology, large language models (LLMs) have become a hot research topic. Education plays an important role in human social development and progress. Traditional education faces challenges such as individual student differences, insufficient allocation of teaching resources, and assessment of teaching effectiveness. Therefore, the applications of LLMs in the field of digital/smart education have broad prospects. The research on educational large models (EduLLMs) is constantly evolving, providing new methods and approaches to achieve personalized learning, intelligent tutoring, and educational assessment goals, thereby improving the quality of education and the learning experience. This article aims to investigate and summarize the application of LLMs in smart education. It first introduces the research background and motivation of LLMs and explains the essence of LLMs. It then discusses the relationship between digital education and EduLLMs and summarizes the current research status of educational large models. The main contributions are the systematic summary and vision of the research background, motivation, and application of large models for education (LLM4Edu). By reviewing existing research, this article provides guidance and insights for educators, researchers, and policy-makers to gain a deep understanding of the potential and challenges of LLM4Edu. It further provides guidance for further advancing the development and application of LLM4Edu, while still facing technical, ethical, and practical challenges requiring further research and exploration. (https://arxiv.org/abs/2311.13160)\\nDocument 2: The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment. (https://arxiv.org/abs/2403.18105)\\nDocument 3: In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education. LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course \"Digital Image Processing\". The research findings show comparable scores between the two groups on the retention test. However, students who engaged in dialogue with ChatGPT exhibited lower performance on the transfer test. Electroencephalography data revealed that students who interacted with ChatGPT exhibited higher levels of cognitive activity, suggesting that ChatGPT could help students establish a knowledge foundation and stimulate cognitive activity. However, its strengths on promoting students. knowledge application and creativity were insignificant. Based upon the research findings, it is evident that ChatGPT cannot fully excel in fulfilling teaching tasks in the dialogue teaching in information related courses. Combining ChatGPT with traditional human teachers might be a more ideal approach. The synergistic use of both can provide students with more comprehensive learning support, thus contributing to enhancing the quality of teaching. (https://arxiv.org/abs/2403.16687)\\nDocument 4: The universal availability of ChatGPT and other similar tools since late 2022 has prompted tremendous public excitement and experimental effort about the potential of large language models (LLMs) to improve learning experience and outcomes, especially for learners from disadvantaged backgrounds. However, little research has systematically examined the real-world impacts of LLM availability on educational equity beyond theoretical projections and controlled studies of innovative LLM applications. To depict trends of post-LLM inequalities, we analyze 1,140,328 academic writing submissions from 16,791 college students across 2,391 courses between 2021 and 2024 at a public, minority-serving institution in the US. We find that students\\' overall writing quality gradually increased following the availability of LLMs and that the writing quality gaps between linguistically advantaged and disadvantaged students became increasingly narrower. However, this equitizing effect was more concentrated on students with higher socioeconomic status. These findings shed light on the digital divides in the era of LLMs and raise questions about the equity benefits of LLMs in early stages and highlight the need for researchers and practitioners on developing responsible practices to improve educational equity through LLMs. (https://arxiv.org/abs/2410.22282)\\nDocument 5: Structured science summaries or research contributions using properties or dimensions beyond traditional keywords enhances science findability. Current methods, such as those used by the Open Research Knowledge Graph (ORKG), involve manually curating properties to describe research papers\\' contributions in a structured manner, but this is labor-intensive and inconsistent between the domain expert human curators. We propose using Large Language Models (LLMs) to automatically suggest these properties. However, it\\'s essential to assess the readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before application. Our study performs a comprehensive comparative analysis between ORKG\\'s manually curated properties and those generated by the aforementioned state-of-the-art LLMs. We evaluate LLM performance through four unique perspectives: semantic alignment and deviation with ORKG properties, fine-grained properties mapping accuracy, SciNCL embeddings-based cosine similarity, and expert surveys comparing manual annotations with LLM outputs. These evaluations occur within a multidisciplinary science setting. Overall, LLMs show potential as recommendation systems for structuring science, but further finetuning is recommended to improve their alignment with scientific tasks and mimicry of human expertise. (https://arxiv.org/abs/2405.02105)'", "args": "('Research articles on LLMs in K-12 education',)", "kwargs": "{}", "model_name": "cross-encoder/ms-marco-MiniLM-L-12-v2", "type": "retriever", "version_name": "retriever/cross-encoder/ms-marco-MiniLM-L-12-v2/v1"}
{"execution_time": 1.4139707909926074, "execution_date": "2025-07-20 03:39:55", "output": "\"Document 1: The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment. (https://arxiv.org/abs/2403.18105)\\nDocument 2: With the recent developments in large language models (LLMs) and their widespread availability through open source models and/or low-cost APIs, several exciting products and applications are emerging, many of which are in the field of STEM educational technology for K-12 and university students. There is a need to evaluate these powerful language models on several benchmarks, in order to understand their risks and limitations. In this short paper, we summarize and analyze the performance of Bard, a popular LLM-based conversational service made available by Google, on the standardized Physics GRE examination. (https://arxiv.org/abs/2312.04613)\\nDocument 3: Due to the remarkable language understanding and generation abilities of large language models (LLMs), their use in educational applications has been explored. However, little work has been done on investigating the pedagogical ability of LLMs in helping students to learn mathematics. In this position paper, we discuss the challenges associated with employing LLMs to enhance students' mathematical problem-solving skills by providing adaptive feedback. Apart from generating the wrong reasoning processes, LLMs can misinterpret the meaning of the question, and also exhibit difficulty in understanding the given questions' rationales when attempting to correct students' answers. Three research questions are formulated. (https://arxiv.org/abs/2310.13615)\\nDocument 4: This paper provides a primer on Large Language Models (LLMs) and identifies their strengths, limitations, applications and research directions. It is intended to be useful to those in academia and industry who are interested in gaining an understanding of the key LLM concepts and technologies, and in utilising this knowledge in both day to day tasks and in more complex scenarios where this technology can enhance current practices and processes. (https://arxiv.org/abs/2412.04503)\\nDocument 5: In recent years, the rapid advancement and impressive capabilities of Large Language Models (LLMs) have been evident across various domains. This paper explores the application, implications, and potential of LLMs in building energy efficiency and decarbonization studies. The wide-ranging capabilities of LLMs are examined in the context of the building energy field, including intelligent control systems, code generation, data infrastructure, knowledge extraction, and education. Despite the promising potential of LLMs, challenges including complex and expensive computation, data privacy, security and copyright, complexity in fine-tuned LLMs, and self-consistency are discussed. The paper concludes with a call for future research focused on the enhancement of LLMs for domain-specific tasks, multi-modal LLMs, and collaborative research between AI and energy experts. (https://arxiv.org/abs/2312.11701)\"", "args": "('research papers on large language models (LLMs) in K-12 education',)", "kwargs": "{}", "model_name": "cross-encoder/ms-marco-MiniLM-L-12-v2", "type": "retriever", "version_name": "retriever/cross-encoder/ms-marco-MiniLM-L-12-v2/v1"}
{"execution_time": 1.2575562080019154, "execution_date": "2025-07-20 23:00:52", "output": "\"Document 1: People with injuries to the peripheral nervous system, due to its poor functional regeneration, suffer from paralysis of the facial muscles, fingers and hands, or toes and feet, often for the rest of their lives. Therefore, to improve patients' quality of life, there is an urgent need for conduits that effectively support the healing of large defects in nerve pathways through specific guidance of nerve cells. This paper describes two specific methods for achieving directed growth of Schwann cells, a type of glial cells that can support the regeneration of the nerve pathway by guiding the neuronal axons in the direction of their alignment. One method implies the exposure of a poly(ethylene terephthalate) (PET) foil to a KrF* laser beam, that renders a nanorippled surface topography. The other method uses aligned polyamide-6 (PA-6) nanofibers produced via electrospinning on a very fast rotating structured collector, which enables easy nanofiber detachment, without additional effort. Schwann cells growth on these substrates was inspected after one week of cultivation by means of scanning electron microscope (SEM). For both methods we show that Schwann cells grow in a certain direction, predetermined by nanoripples and nanofibers orientation. In contrast, cells cultivated onto unstructured surfaces or randomly oriented nanofibers, show an omnidirectional growth behavior. (https://arxiv.org/abs/2403.01973)\\nDocument 2: Maintaining tissue homeostasis requires appropriate regulation of stem cell differentiation. The Waddington landscape posits that gene circuits in a cell form a potential landscape of different cell types, wherein cells follow attractors of the probability landscape to develop into distinct cell types. However, how adult stem cells achieve a delicate balance between self-renewal and differentiation remains unclear. We propose that random inheritance of epigenetic states plays a pivotal role in stem cell differentiation and present a hybrid model of stem cell differentiation induced by epigenetic modifications. Our comprehensive model integrates gene regulation networks, epigenetic state inheritance, and cell regeneration, encompassing multi-scale dynamics ranging from transcription regulation to cell population. Through model simulations, we demonstrate that random inheritance of epigenetic states during cell divisions can spontaneously induce cell differentiation, dedifferentiation, and transdifferentiation. Furthermore, we investigate the influences of interfering with epigenetic modifications and introducing additional transcription factors on the probabilities of dedifferentiation and transdifferentiation, revealing the underlying mechanism of cell reprogramming. This \\\\textit{in silico} model provides valuable insights into the intricate mechanism governing stem cell differentiation and cell reprogramming and offers a promising path to enhance the field of regenerative medicine. (https://arxiv.org/abs/2309.07356)\\nDocument 3: Lampreys are one of the oldest species in the world, living longer than dinosaurs, which is related to the ability to change the sex ratio during their lifespan. In this paper, to understand how sex ratio and food quantity affect the population growth rate of lampreys, the researchers draw inspiration from the logistics model and established a model called EcoSexChange(ESC), which results in a population initially increasing and then stabilizing, a reasonable outcome that may apply to other organisms with significant differences in consumption between sexes. Subsequently, this paper develops the Sex Ratio Adaptation Eco Impact (SRAEI) model based on the ESC model using the ABM algorithm to simulate how the population of lampreys, whose lives are divided into seven stages, grows and stabilizes. Then introduces a sudden disaster factor in the middle of the simulation, while also comparing lampreys that cannot adjust their sex ratio. The results of this paper are of great reference significance for people to analyze the population changes of lampreys in different living environments, and they are also easy to apply to other species with large differences between males and females. (https://arxiv.org/abs/2407.10411)\\nDocument 4: This paper reviews work published between 2002 and 2022 in the fields of Android malware, clone, and similarity detection. It examines the data sources, tools, and features used in existing research and identifies the need for a comprehensive, cross-domain dataset to facilitate interdisciplinary collaboration and the exploitation of synergies between different research areas. Furthermore, it shows that many research papers do not publish the dataset or a description of how it was created, making it difficult to reproduce or compare the results. The paper highlights the necessity for a dataset that is accessible, well-documented, and suitable for a range of applications. Guidelines are provided for this purpose, along with a schematic method for creating the dataset. (https://arxiv.org/abs/2412.11539)\\nDocument 5: With the Open Science approach becoming important for research, the evolution towards open scientific-paper reviews is making an impact on the scientific community. However, there is a lack of publicly available resources for conducting research activities related to this subject, as only a limited number of journals and conferences currently allow access to their review process for interested parties. In this paper, we introduce the new comprehensive Open Review-Based dataset (ORB); it includes a curated list of more than 36,000 scientific papers with their more than 89,000 reviews and final decisions. We gather this information from two sources: the OpenReview.net and SciPost.org websites. However, given the volatile nature of this domain, the software infrastructure that we introduce to supplement the ORB dataset is designed to accommodate additional resources in the future. The ORB deliverables include (1) Python code (interfaces and implementations) to translate document data and metadata into a structured and high-level representation, (2) an ETL process (Extract, Transform, Load) to facilitate the automatic updates from defined sources and (3) data files representing the structured data. The paper presents our data architecture and an overview of the collected data along with relevant statistics. For illustration purposes, we also discuss preliminary Natural-Language-Processing-based experiments that aim to predict (1) papers' acceptance based on their textual embeddings, and (2) grading statistics inferred from embeddings as well. We believe ORB provides a valuable resource for researchers interested in open science and review, with our implementation easing the use of this data for further analysis and experimentation. We plan to update ORB as the field matures as well as introduce new resources even more fitted to dedicated scientific domains such as High-Energy Physics. (https://arxiv.org/abs/2312.04576)\"", "args": "('research papers on axolotl skin regeneration',)", "kwargs": "{}", "model_name": "cross-encoder/ms-marco-MiniLM-L-12-v2", "type": "retriever", "version_name": "retriever/cross-encoder/ms-marco-MiniLM-L-12-v2/v1"}
{"execution_time": 1.3553909999900497, "execution_date": "2025-07-21 20:18:50", "output": "'Document 1: Large language models are ubiquitous in natural language processing because they can adapt to new tasks without retraining. However, their sheer scale and complexity present unique challenges and opportunities, prompting researchers and practitioners to explore novel model training, optimization, and deployment methods. This literature review focuses on various techniques for reducing resource requirements and compressing large language models, including quantization, pruning, knowledge distillation, and architectural optimizations. The primary objective is to explore each method in-depth and highlight its unique challenges and practical applications. The discussed methods are categorized into a taxonomy that presents an overview of the optimization landscape and helps navigate it to understand the research trajectory better. (https://arxiv.org/abs/2408.03130)\\nDocument 2: This paper presents novel systems and methodologies for the development of efficient large language models (LLMs). It explores the trade-offs between model size, performance, and computational resources, with the aim of maximizing the efficiency of these AI systems. The research explores novel methods that allow different parts of the model to share parameters, reducing the total number of unique parameters required. This approach ensures that the model remains compact without sacrificing its ability to learn and represent complex language structures. This study provides valuable insights and tools for creating more efficient and effective LLMs, contributing to a more sustainable and accessible future for AI language modeling. (https://arxiv.org/abs/2309.06589)\\nDocument 3: Despite advancements in English-dominant generative large language models, further development is needed for low-resource languages to enhance global accessibility. The primary methods for representing these languages are monolingual and multilingual pretraining. Monolingual pretraining is expensive due to hardware requirements, and multilingual models often have uneven performance across languages. This study explores an alternative solution by adapting large language models, primarily trained on English, to low-resource languages. We assess various strategies, including continual training, instruction fine-tuning, task-specific fine-tuning, and vocabulary extension. The results show that continual training improves language comprehension, as reflected in perplexity scores, and task-specific tuning generally enhances performance of downstream tasks. However, extending the vocabulary shows no substantial benefits. Additionally, while larger models improve task performance with few-shot tuning, multilingual models perform worse than their monolingual counterparts when adapted. (https://arxiv.org/abs/2405.07745)\\nDocument 4: Large Language Models are expressive tools that enable complex tasks of text understanding within Computational Social Science. Their versatility, while beneficial, poses a barrier for establishing standardized best practices within the field. To bring clarity on the values of different strategies, we present an overview of the performance of modern LLM-based classification methods on a benchmark of 23 social knowledge tasks. Our results point to three best practices: select models with larger vocabulary and pre-training corpora; avoid simple zero-shot in favor of AI-enhanced prompting; fine-tune on task-specific data, and consider more complex forms instruction-tuning on multiple datasets only when only training data is more abundant. (https://arxiv.org/abs/2408.01346)\\nDocument 5: With the recent developments in large language models (LLMs) and their widespread availability through open source models and/or low-cost APIs, several exciting products and applications are emerging, many of which are in the field of STEM educational technology for K-12 and university students. There is a need to evaluate these powerful language models on several benchmarks, in order to understand their risks and limitations. In this short paper, we summarize and analyze the performance of Bard, a popular LLM-based conversational service made available by Google, on the standardized Physics GRE examination. (https://arxiv.org/abs/2312.04613)'", "args": "('Learning resources for Large Language Models',)", "kwargs": "{}", "model_name": "cross-encoder/ms-marco-MiniLM-L-12-v2", "type": "retriever", "version_name": "retriever/cross-encoder/ms-marco-MiniLM-L-12-v2/v1"}
